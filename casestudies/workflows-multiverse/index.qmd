---
title: "Iterative filtering for multiverse analyses of treatment effects"
date: 2024-04-05
author: 
  - name: 
      given: Anna Elisabeth
      family: Riha
    url: https://anna.riha.github.io
    orcid: 0009-0003-8396-906X
bibliography: references.bib
format: 
  html:
    toc: true
    toc-depth: 3
categories: [multiverse analysis, Bayesian workflows, brms]
citation: true
---

<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 10px;
margin-right: 25px;
border-radius: 5px;
font-style: italic;
}
</style>

```{css}
#| echo: false
p {
  text-align: justify
}
```

```{r}
#| message: false
#| warning: false
#| output: false
#| code-fold: true

library(here)
library(readr)
library(tictoc)
library(knitr)
library(kableExtra)
library(tinytable)
library(reactable)
library(htmltools)
library(tidyverse)
library(brms)
library(future)
library(furrr)
library(cmdstanr)
library(ggplot2)
library(ggdist)
```

<p class="comment">
Can we combine **transparent creation of sets of models** (multiverse analysis) with **recipes for model building and evaluation** (Bayesian workflows) to support Bayesian modelling?
<p>

Bayesian modelling workflows consist of several intertwined tasks and can involve the iterative consideration of various candidate models (see, e.g., @gelmanetal2020, @martinetal2021). 
Aspects like computation checks, model evaluation, model criticism and model comparison can motivate the consideration of multiple models and are essential when searching for models that are sufficient for obtaining accurate predictions and enabling robust decision-making (see, e.g., @ohaganbayesian2004, @vehtariojanen2012, @piironenvehtari2017, @burknermodels2023).

Multiverse analysis provides a framework to transparently investigate several models at once (@steegenetal2016). 
But reasoning on a set of models can be challenging, and dependence structures and different weights of modelling choices are not immediately clear when confronted with a large collection of possible models (see e.g., @halletal2022). 

In this [preprint](https://arxiv.org/abs/2404.01688), we propose iterative filtering for multiverse analysis to balance the advantages of a joint investigation of multiple candidate models with the potentially overwhelming tasks of evaluating and comparing multiple models at once. 

## Analysing an anticonvulsant for patients with epilepsy

Let's see how iterative filtering can be applied to multiverse analyses of anticonvulsant therapy for patients with epilepsy. 

We use the dataset `brms::epilepsy` from the `brms` package [@buerknerbrms2017] with `r NROW(brms::epilepsy)` observations of `r length(unique(brms::epilepsy$patient))` patients.
It was initially published by @leppikcontrolled1987, and previously analysed, for example, by @thallcovariance1990 and @breslowapproximate1993.

The data contains information on:

-   $\texttt{Trt}$: 0 or 1 if patient received anticonvulsant therapy
-   $\texttt{Age}$: age of patients in years
-   $\texttt{Base}$: seizure count at 8-week baseline
-   $\texttt{zAge}$: standardised age
-   $\texttt{zBase}$: standardised baseline
-   $\texttt{patient}$: patient number
-   $\texttt{visit}$: session number from 1 (first visit) to 4 (last visit)
-   $\texttt{obs}$: unique identifier for each observation
-   $\texttt{count}$: seizure count between two visits.

Here is a quick glimpse: 
```{r}
#| code-fold: true

tt(head(brms::epilepsy, 3))
```

## An initial multiverse of models 

To analyse the effect of anticonvulsant therapy on seizure counts, we choose models with Poisson and negative Binomial distributional families for the observations because they are suitable for non-negative integers. 
Additionally, we want to investigate default prior settings in `brms` as well as models with a horseshoe prior with three degrees of freedom for the population-level effects. 
Additionally, we evaluate different combinations of covariates as well as models with and without interaction effect `zBase*Trt`. The combination of these modelling choices leads to $2 \times 2 \times 6 = 24$ candidate models.  

```{r}
#| code-fold: show

# create dataframe of combinations of model components ####
combinations_df <- expand.grid(
  family = names(list(poisson = poisson(), negbinomial = negbinomial())),
  prior = list(brms_default = "NULL", brms_horseshoe = "horseshoe(3)"),
  # population-level effects
  Trt = c("", "Trt"), 
  zBase = c("", "zBase"),
  zAge = c("", "zAge")
)

combinations_df <- combinations_df |> 
  # add interaction effect
  mutate(zBaseTrt = factor(
    case_when(
      Trt == "Trt" ~ "",
      Trt == "" ~ "zBase * Trt"))) |> 
  # filter out rows with interaction and zBase
  filter(!(zBaseTrt == "zBase * Trt" & combinations_df$zBase == "zBase"))

outcome_str <- "count" 

combinations_df <- combinations_df |>  
  # add outcome name 
  mutate(outcome = rep_len(outcome_str, NROW(combinations_df))) |>
  # add prior names for easier summarising, plotting etc. 
  mutate(priors = names(combinations_df$prior)) |>
  # reorder to have outcome name, family and treatment effects first 
  select(outcome, family, priors, prior, Trt, zBaseTrt, everything())
```

For the sake of simplicity, we do not fit all the models here but only show the code to obtain the modelfits and load a dataframe containing the modelfits for all 24 models.

```{r}
#| code-fold: show
# load results for an initial multiverse of 24 models 
initial_multiverse <- readr::read_rds(here::here("data", "initial_multiverse.rds"))
```

Below is the code that generates this dataframe.
We set `#| eval: false` in the chunk options since we are not evaluating the code chunk here. 

```{r}
#| code-fold: show
#| eval: false

initial_multiverse <- combinations_df |>
  mutate(modelnames = apply(combinations_df, 1, build_name))

# workhorse: fit models ####
tic()
future::plan(multisession, workers = parallel::detectCores()-1)
initial_multiverse$modelfits <- combinations_df |>
  group_nest(row_number()) |>
  pull(data) |>
  furrr::future_map(~build_fit(.x, dataset = brms::epilepsy), .options=furrr_options(seed=TRUE))
future::plan(sequential)
toc()

# add draws df ####
initial_multiverse <- initial_multiverse |>
  mutate(model_id = paste0("Model ", row_number())) |>
  mutate(draws_df = purrr::map(purrr::map(modelfits, pluck), posterior::as_draws_df))
```

To fit the models, we use the below helper functions `build_name()`, `build_brms_formula()` and `build_fit()` for each row vector of modelling choices recorded in the initial dataframe. 
We set `#| eval: false` in the chunk options since we are not evaluating the code chunk here.

```{r}
#| code-fold: true
#| eval: false

build_name <- function(row, ...){
  outcome = row[["outcome"]]
  # prior names
  priornames = row[["priors"]]
  in_id <- c(which(!(names(row) %in% c("outcome", "family", "prior", "priors")) & row != ""))
  # cells that are included in the formula
  covars <- row[in_id]
  # extract levels for formula
  covars <- as.character(unlist(covars))
  # paste formula
  formula1 = paste(outcome, "~", paste(covars, collapse = "+")) 
  # build name
  name = paste0(row[["family"]], "(", formula1, "), ", priornames)
  out <- name
}

build_brms_formula <- function(row, ...){
  outcome = row[["outcome"]]
  fam = as.character(unlist(row["family"]))
  in_id <- c(which(!(names(row) %in% c("outcome", "family", "prior", "priors", "model_name")) & row != ""))
  # cells that are included in the formula
  covars <- row[in_id]
  # extract levels for formula
  covars <- as.character(unlist(covars))
  # paste formula
  formula_str = paste(outcome, "~", paste(covars, collapse = "+")) 
  # turn string into formula 
  formula = brms::brmsformula(as.formula(formula_str), family=fam)
  out <- formula 
} 

build_fit <- function(row, dataset, ...){
  # set priors 
  if (row[["priors"]] == "brms_horseshoe"){
    prior = brms::set_prior("horseshoe(3)")
  } else if (row[["priors"]] == "brms_default"){
    prior = NULL
  }
  # fit model with brms
  brm(
    formula = build_brms_formula(row), 
    data = dataset, 
    prior = prior,
    seed = 424242,
    backend = "cmdstanr", 
    silent = 2, 
    refresh = 0
  ) 
}
```

## Evaluating the multiverse 

We use the `loo` package [@vehtariloo2020] to obtain estimates of expected log predictive densities (elpd) with PSIS-LOO-CV using `loo::loo()` and compare the predictive abilities of the models using `loo::loo_compare()`. 
For the purpose of this illustration, we load the loo-objects for all models that have been previously obtained and just present the code that was used to get the results for all models below.

```{r}
#| code-fold: show
#| eval: false 

loos_default <- readr::read_rds(here::here("data", "loos_default.rds"))
```

Again, we set `#| eval: false` in the chunk options since we are not evaluating the code chunk here.

```{r}
#| code-fold: show
#| eval: false

# workhorse: default PSIS-LOO-CV for all models ####
tic()
future::plan(multisession, workers = parallel::detectCores()-1)
loos_default <- initial_multiverse |>
  group_nest(row_number()) |>
  pull(data) |>
  furrr::future_map(~build_loos(.x, dataset = brms::epilepsy), .options=furrr_options(seed=TRUE))
toc()
future::plan(sequential)

# set names for loo objects
names(loos_default) <- initial_multiverse$modelnames
```

The above code uses the following helper function `build_loos()` as a wrapper around `loo::loo()` to obtain estimates for elpd with PSIS-LOO-CV for one row in `initial_multiverse`. 

```{r}
#| code-fold: true
#| eval: false

# loo: elpd and model comparison ####
build_loos <- function(row, dataset, ...){
  modelfit = row[["modelfits"]][[1]]
  loo_object = loo(modelfit)
  return(loo_object)
} 
```

## Filtering by predictive abilities 

We filter out models with largely inferior predictive abilities using estimated elpd and visual posterior predictive checks. 

## What comes next? 

In part II of this case study, we will extend the filtered set of models by including more complex models and filter again. 
We will also show how we can use integrated PSIS-LOO-CV to obtain reliable estimates for elpd. 

## Appendix

### Existing work

-   posterior calibration checks with @saeilynoja2022
-   model comparison with $\texttt{R}$-package `loo` [@vehtariloo2020]
-   multiverse analysis [@steegenetal2016] and `multiverse` $\texttt{R}$-package [@sarmaetal2021]
-   explorable multiverse analyses [@dragicevic2019]
-   creating multiverse analysis scripts, exploring results with Boba [@liu2021]
-   survey of visualisation of multiverse analyses [@halletal2022]
-   modular STAN [@bernstein2020]
-   modelling multiverse analysis for machine learning [@belletal2022]

[^2]: <https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html#1_Questioning_Authority>

The following figures show two variants of flowcharts for Bayesian workflows with different levels of detail[^4]. The most apparent similarities are (1) the possibility to iterate when needed, and (2) connecting the tasks of modelling and checking and tending to computational issues.

[^4]: Another flowchart for Bayesian workflow can be found in Michael Betancourt's [blogpost](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html#1_Questioning_Authority) "Principled Bayesian Workflow".

::: {#fig-workflows layout=[[2.25,3.1]]}
![Bayesian Workflow in [@gelmanetal2020]](bayesworkflow-illu.png){#fig-bw-gelman}

![Bayesian Workflow in [@martinetal2021].](workflow-bayescompbook.png){#fig-bw-martin}
:::

### Challenges in Bayesian workflows

-   multi-attribute multi-objective scenarios
-   navigating necessary vs. "nice-to-have" steps
-   stopping criteria and sufficient exploration
-   iterative model building, while transparent and robust
-   communicating results of multiple models

### Transparent Exploration with multiverse analysis


::: {#fig-multiverse layout=[[3,1]]}
![Multiverse analysis compared to other approaches, from [@dragicevic2019].](multiverse-illu.png){#fig-mvd width=70%}

![Multiverse analysis in Bayesian workflow in [@gelmanetal2020].](multiverse-in-bw.png){#fig-mvbw width=70%}
:::

Multiverse analysis provides a way to transparently define and fit several models at once (@steegenetal2016). 
In a workflow that requires iterations, this could allow parallel exploration, thereby, increasing efficiency. 
On the other hand, this exploration of sets of models necessarily depends on researcher/data analyst/user choices, and is subject to computational and cognitive constraints. 

Reasoning on a set of models can be challenging, and dependence structures and different weights of modelling choices are not immediately clear when confronted with a large collection of possible models (see e.g., @halletal2022). 

### Differences and structure in a set of models 

Given a set of $m$ models $\mathcal{M} = \{M_1, M_2, ..., M_m\}$, let $C_1, \cdots, C_k$ denote $k$ different modelling choices. If, for example, $C_1 = \{\text{"poisson"}, \text{"negbinomial"}\}$, $C_2 = \{\text{"Trt"}\}$ and $C_3 = \{ \text{"no zAge"}, \text{"zAge"} \}$, one could draw networks of the resulting four models solely based on how much they differ in each of the conditions. 

Below, the left-hand side shows one step differences, while the right-hand side includes two step differences for models created using the above modelling choices $C_1, C_2$ and $C_3$.

::: {layout-ncol="2"}
```{dot}
//| fig-width: 4
graph D {
  
  A [shape=box,label="Poisson(Trt)"]
  B [shape=box,label="Poisson(Trt+zAge)"]
  C [shape=box,label="Negbinom(Trt)"]
  D [shape=box,label="Negbinom(Trt+zAge)"]
  
  A -- B 
  A -- C 
  C -- D 
  B -- D 

}
```

```{dot}
//| fig-width: 3

graph D {
  
  A [shape=box, label="Poisson(Trt)"]
  B [shape=box, label="Poisson(Trt+zAge)"]
  C [shape=box, label="Negbinom(Trt)"]
  D [shape=box, label="Negbinom(Trt+zAge)"]
  
  A -- B -- D
  A -- C -- D
  C -- B [color="grey:invis:grey"]
  D -- A [color="grey:invis:grey"]

}
```
:::
