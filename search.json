[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code | Blogpost | StanCon 2024 Talk\nWhen building statistical models for Bayesian data analysis tasks, required and optional iterative adjustments and different modelling choices can give rise to numerous candidate models. Checks and evaluations throughout the modelling process can motivate changes to an existing model or the consideration of alternatives. Failing to consider alternative models can lead to overconfidence in the predictive or inferential ability of a chosen model. The search for suitable models requires modellers to work with multiple models without jeopardising the validity of their results. Multiverse analysis enables the transparent creation of several models based on different modelling choices, but the number of models can become overwhelming in practice, and we require tools to reduce sets of models towards fewer models of higher quality across different modelling contexts. Motivated by these challenges, this work proposes iterative filtering for multiverse analysis to support efficient and consistent assessment of multiple models. Given that causal constraints have been considered, we show how multiverse analysis can be combined with recommendations from established Bayesian modelling workflows to identify promising candidate models by assessing predictive abilities and, if needed, tending to computational issues. We illustrate our suggested approach in different realistic modelling scenarios using real data examples."
  },
  {
    "objectID": "projects/index.html#supporting-bayesian-workflows-with-iterative-filtering-for-multiverse-analysis",
    "href": "projects/index.html#supporting-bayesian-workflows-with-iterative-filtering-for-multiverse-analysis",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code | Blogpost | StanCon 2024 Talk\nWhen building statistical models for Bayesian data analysis tasks, required and optional iterative adjustments and different modelling choices can give rise to numerous candidate models. Checks and evaluations throughout the modelling process can motivate changes to an existing model or the consideration of alternatives. Failing to consider alternative models can lead to overconfidence in the predictive or inferential ability of a chosen model. The search for suitable models requires modellers to work with multiple models without jeopardising the validity of their results. Multiverse analysis enables the transparent creation of several models based on different modelling choices, but the number of models can become overwhelming in practice, and we require tools to reduce sets of models towards fewer models of higher quality across different modelling contexts. Motivated by these challenges, this work proposes iterative filtering for multiverse analysis to support efficient and consistent assessment of multiple models. Given that causal constraints have been considered, we show how multiverse analysis can be combined with recommendations from established Bayesian modelling workflows to identify promising candidate models by assessing predictive abilities and, if needed, tending to computational issues. We illustrate our suggested approach in different realistic modelling scenarios using real data examples."
  },
  {
    "objectID": "casestudies/workflows-multiverse/index.html",
    "href": "casestudies/workflows-multiverse/index.html",
    "title": "Iterative filtering for multiverse analyses of treatment effects",
    "section": "",
    "text": "Can we combine transparent creation of sets of models (multiverse analysis) with recipes for model building and evaluation (Bayesian workflows) to support Bayesian modelling?\nBayesian modelling workflows consist of several intertwined tasks and can involve the iterative consideration of various candidate models (see, e.g., Gelman et al. (2020), Martin, Kumar, and Lao (2021)). Aspects like computation checks, model evaluation, model criticism and model comparison can motivate the consideration of multiple models and are essential when searching for models that are sufficient for obtaining accurate predictions and enabling robust decision-making (see, e.g., O’Hagan and Forster (2004), Vehtari and Ojanen (2012), Piironen and Vehtari (2017), Bürkner, Scholz, and Radev (2023)).\nMultiverse analysis provides a framework to transparently investigate several models at once (Steegen et al. (2016)). But reasoning on a set of models can be challenging, and dependence structures and different weights of modelling choices are not immediately clear when confronted with a large collection of possible models (see e.g., Hall et al. (2022)).\nIn this preprint, we propose iterative filtering for multiverse analysis to balance the advantages of a joint investigation of multiple candidate models with the potentially overwhelming tasks of evaluating and comparing multiple models at once."
  },
  {
    "objectID": "casestudies/workflows-multiverse/index.html#analysing-an-anticonvulsant-for-patients-with-epilepsy",
    "href": "casestudies/workflows-multiverse/index.html#analysing-an-anticonvulsant-for-patients-with-epilepsy",
    "title": "Iterative filtering for multiverse analyses of treatment effects",
    "section": "Analysing an anticonvulsant for patients with epilepsy",
    "text": "Analysing an anticonvulsant for patients with epilepsy\n\n\nCode\nlibrary(here)\nlibrary(readr)\nlibrary(tictoc)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tinytable)\nlibrary(reactable)\nlibrary(htmltools)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(future)\nlibrary(furrr)\nlibrary(cmdstanr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(patchwork)\nlibrary(latex2exp)\nlibrary(bayesplot)\n\n\nLet’s see how iterative filtering can be applied to multiverse analyses of anticonvulsant therapy for patients with epilepsy.\nWe use the dataset brms::epilepsy from the brms package (Bürkner 2017) with 236 observations of 59 patients. It was initially published by Leppik et al. (1987), and previously analysed, for example, by Thall and Vail (1990) and Breslow and Clayton (1993).\nThe data contains information on:\n\n\\(\\texttt{Trt}\\): 0 or 1 if patient received anticonvulsant therapy\n\\(\\texttt{Age}\\): age of patients in years\n\\(\\texttt{Base}\\): seizure count at 8-week baseline\n\\(\\texttt{zAge}\\): standardised age\n\\(\\texttt{zBase}\\): standardised baseline\n\\(\\texttt{patient}\\): patient number\n\\(\\texttt{visit}\\): session number from 1 (first visit) to 4 (last visit)\n\\(\\texttt{obs}\\): unique identifier for each observation\n\\(\\texttt{count}\\): seizure count between two visits.\n\nHere is a quick glimpse:\n\n\nCode\ntt(head(brms::epilepsy, 3))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Age\n                Base\n                Trt\n                patient\n                visit\n                count\n                obs\n                zAge\n                zBase\n              \n        \n        \n        \n                \n                  31\n                  11\n                  0\n                  1\n                  1\n                  5\n                  1\n                  0.4249950\n                  -0.7571728\n                \n                \n                  30\n                  11\n                  0\n                  2\n                  1\n                  3\n                  2\n                  0.2652835\n                  -0.7571728\n                \n                \n                  25\n                  6\n                  0\n                  3\n                  1\n                  2\n                  3\n                  -0.5332740\n                  -0.9444033"
  },
  {
    "objectID": "casestudies/workflows-multiverse/index.html#an-initial-multiverse-of-models",
    "href": "casestudies/workflows-multiverse/index.html#an-initial-multiverse-of-models",
    "title": "Iterative filtering for multiverse analyses of treatment effects",
    "section": "An initial multiverse of models",
    "text": "An initial multiverse of models\nTo analyse the effect of anticonvulsant therapy on seizure counts, we choose models with Poisson and negative Binomial distributional families for the observations because they are suitable for non-negative integers. Additionally, we want to investigate default prior settings in brms as well as models with a horseshoe prior with three degrees of freedom for the population-level effects. Additionally, we evaluate different combinations of covariates as well as models with and without interaction effect zBase*Trt. The combination of these modelling choices leads to \\(2 \\times 2 \\times 6 = 24\\) candidate models.\n\n\nCode\n# create dataframe of combinations of model components ####\ncombinations_df &lt;- expand.grid(\n  family = names(list(poisson = poisson(), negbinomial = negbinomial())),\n  prior = list(brms_default = \"NULL\", brms_horseshoe = \"horseshoe(3)\"),\n  # population-level effects\n  Trt = c(\"\", \"Trt\"), \n  zBase = c(\"\", \"zBase\"),\n  zAge = c(\"\", \"zAge\")\n)\n\ncombinations_df &lt;- combinations_df |&gt; \n  # add interaction effect\n  mutate(zBaseTrt = factor(\n    case_when(\n      Trt == \"Trt\" ~ \"\",\n      Trt == \"\" ~ \"zBase * Trt\"))) |&gt; \n  # filter out rows with interaction and zBase\n  filter(!(zBaseTrt == \"zBase * Trt\" & combinations_df$zBase == \"zBase\"))\n\noutcome_str &lt;- \"count\" \n\ncombinations_df &lt;- combinations_df |&gt;  \n  # add outcome name \n  mutate(outcome = rep_len(outcome_str, NROW(combinations_df))) |&gt;\n  # add prior names for easier summarising, plotting etc. \n  mutate(priors = names(combinations_df$prior)) |&gt;\n  # reorder to have outcome name, family and treatment effects first \n  select(outcome, family, priors, prior, Trt, zBaseTrt, everything())\n\n\nFor the sake of simplicity, we do not fit all the models here but only show the code to obtain the modelfits and load a dataframe containing the modelfits for all 24 models.\n\n\nCode\n# load results for an initial multiverse of 24 models \ninitial_multiverse &lt;- readr::read_rds(here::here(\"data\", \"initial_multiverse.rds\"))\n\n\nBelow is the code that generates this dataframe. We set #| eval: false in the chunk options since we are not evaluating the code chunk here.\n\n\nCode\ninitial_multiverse &lt;- combinations_df |&gt;\n  mutate(modelnames = apply(combinations_df, 1, build_name))\n\n# workhorse: fit models ####\ntic()\nfuture::plan(multisession, workers = parallel::detectCores()-1)\ninitial_multiverse$modelfits &lt;- combinations_df |&gt;\n  group_nest(row_number()) |&gt;\n  pull(data) |&gt;\n  furrr::future_map(~build_fit(.x, dataset = brms::epilepsy), .options=furrr_options(seed=TRUE))\nfuture::plan(sequential)\ntoc()\n\n# add draws df ####\ninitial_multiverse &lt;- initial_multiverse |&gt;\n  mutate(model_id = paste0(\"Model \", row_number())) |&gt;\n  mutate(draws_df = purrr::map(purrr::map(modelfits, pluck), posterior::as_draws_df))\n\n\nTo fit the models, we use the below helper functions build_name(), build_brms_formula() and build_fit() for each row vector of modelling choices recorded in the initial dataframe. We set #| eval: false in the chunk options since we are not evaluating the code chunk here.\n\n\nCode\nbuild_name &lt;- function(row, ...){\n  outcome = row[[\"outcome\"]]\n  # prior names\n  priornames = row[[\"priors\"]]\n  in_id &lt;- c(which(!(names(row) %in% c(\"outcome\", \"family\", \"prior\", \"priors\")) & row != \"\"))\n  # cells that are included in the formula\n  covars &lt;- row[in_id]\n  # extract levels for formula\n  covars &lt;- as.character(unlist(covars))\n  # paste formula\n  formula1 = paste(outcome, \"~\", paste(covars, collapse = \"+\")) \n  # build name\n  name = paste0(row[[\"family\"]], \"(\", formula1, \"), \", priornames)\n  out &lt;- name\n}\n\nbuild_brms_formula &lt;- function(row, ...){\n  outcome = row[[\"outcome\"]]\n  fam = as.character(unlist(row[\"family\"]))\n  in_id &lt;- c(which(!(names(row) %in% c(\"outcome\", \"family\", \"prior\", \"priors\", \"model_name\")) & row != \"\"))\n  # cells that are included in the formula\n  covars &lt;- row[in_id]\n  # extract levels for formula\n  covars &lt;- as.character(unlist(covars))\n  # paste formula\n  formula_str = paste(outcome, \"~\", paste(covars, collapse = \"+\")) \n  # turn string into formula \n  formula = brms::brmsformula(as.formula(formula_str), family=fam)\n  out &lt;- formula \n} \n\nbuild_fit &lt;- function(row, dataset, ...){\n  # set priors \n  if (row[[\"priors\"]] == \"brms_horseshoe\"){\n    prior = brms::set_prior(\"horseshoe(3)\")\n  } else if (row[[\"priors\"]] == \"brms_default\"){\n    prior = NULL\n  }\n  # fit model with brms\n  brm(\n    formula = build_brms_formula(row), \n    data = dataset, \n    prior = prior,\n    seed = 424242,\n    backend = \"cmdstanr\", \n    silent = 2, \n    refresh = 0\n  ) \n}"
  },
  {
    "objectID": "casestudies/workflows-multiverse/index.html#evaluating-the-multiverse",
    "href": "casestudies/workflows-multiverse/index.html#evaluating-the-multiverse",
    "title": "Iterative filtering for multiverse analyses of treatment effects",
    "section": "Evaluating the multiverse",
    "text": "Evaluating the multiverse\nWe use the loo package (Vehtari et al. 2020) to obtain estimates of expected log predictive densities (elpd) with PSIS-LOO-CV using loo::loo(). For the purpose of this illustration, we load the loo-objects for all models that have been previously obtained and just present the code that was used to get the results for all models below.\n\n\nCode\nloos_default &lt;- readr::read_rds(here::here(\"data\", \"loos_default.rds\"))\n\n\nAgain, we set #| eval: false in the chunk options since we are not evaluating the code chunk here.\n\n\nCode\n# workhorse: default PSIS-LOO-CV for all models ####\ntic()\nfuture::plan(multisession, workers = parallel::detectCores()-1)\nloos_default &lt;- initial_multiverse |&gt;\n  group_nest(row_number()) |&gt;\n  pull(data) |&gt;\n  furrr::future_map(~build_loos(.x, dataset = brms::epilepsy), .options=furrr_options(seed=TRUE))\ntoc()\nfuture::plan(sequential)\n\n# set names for loo objects\nnames(loos_default) &lt;- initial_multiverse$modelnames\n\n\nThe above code uses the following helper function build_loos() as a wrapper around loo::loo() to obtain estimates for elpd with PSIS-LOO-CV for one row in initial_multiverse.\n\n\nCode\n# loo: elpd and model comparison ####\nbuild_loos &lt;- function(row, dataset, ...){\n  modelfit = row[[\"modelfits\"]][[1]]\n  loo_object = loo(modelfit)\n  return(loo_object)\n} \n\n\nWe compare models in the set of models \\(M = \\{M_1, \\cdots, M_K\\}\\) with \\(K = 24\\) using the difference in estimated \\(\\mathrm{elpd}^k\\) of each model \\(M_k\\) compared to the model with the highest estimated . Given the estimates of elpd for all 24 models, we assess differences in elpd and associated standard errors of the differences for each model using loo::loo_compare().\n\ncomparisons_df &lt;- loo::loo_compare(loos_default)\n\nWarning: Difference in performance potentially due to chance.See McLatchie and\nVehtari (2023) for details.\n\ncomparisons_df\n\n                                                      elpd_diff se_diff\nnegbinomial(count ~ Trt+zBase+zAge), brms_default        0.0       0.0 \nnegbinomial(count ~ Trt+zBase+zAge), brms_horseshoe     -0.2       0.6 \nnegbinomial(count ~ zBase * Trt+zAge), brms_horseshoe   -0.8       0.6 \nnegbinomial(count ~ zBase * Trt+zAge), brms_default     -1.2       0.2 \nnegbinomial(count ~ Trt+zBase), brms_default            -1.5       1.9 \nnegbinomial(count ~ Trt+zBase), brms_horseshoe          -1.6       2.0 \nnegbinomial(count ~ zBase * Trt), brms_horseshoe        -2.0       2.0 \nnegbinomial(count ~ zBase * Trt), brms_default          -2.1       1.9 \nnegbinomial(count ~ Trt), brms_horseshoe               -88.2      14.6 \nnegbinomial(count ~ Trt+zAge), brms_horseshoe          -88.3      14.2 \nnegbinomial(count ~ Trt), brms_default                 -88.9      14.8 \nnegbinomial(count ~ Trt+zAge), brms_default            -88.9      14.2 \npoisson(count ~ Trt+zBase+zAge), brms_default         -211.1      75.4 \npoisson(count ~ zBase * Trt+zAge), brms_default       -212.0      76.0 \npoisson(count ~ Trt+zBase+zAge), brms_horseshoe       -212.1      76.2 \npoisson(count ~ zBase * Trt+zAge), brms_horseshoe     -212.2      76.2 \npoisson(count ~ Trt+zBase), brms_default              -223.6      78.1 \npoisson(count ~ Trt+zBase), brms_horseshoe            -224.4      78.5 \npoisson(count ~ zBase * Trt), brms_horseshoe          -225.3      78.5 \npoisson(count ~ zBase * Trt), brms_default            -226.0      78.4 \npoisson(count ~ Trt), brms_default                    -996.4     239.4 \npoisson(count ~ Trt), brms_horseshoe                  -997.1     239.1 \npoisson(count ~ Trt+zAge), brms_default               -999.2     234.5 \npoisson(count ~ Trt+zAge), brms_horseshoe             -999.6     233.9"
  },
  {
    "objectID": "casestudies/workflows-multiverse/index.html#filtering-with-predictive-density-estimates",
    "href": "casestudies/workflows-multiverse/index.html#filtering-with-predictive-density-estimates",
    "title": "Iterative filtering for multiverse analyses of treatment effects",
    "section": "Filtering with predictive density estimates",
    "text": "Filtering with predictive density estimates\nTo filter out models with largely inferior predictive abilities, we can identify a set of models with indistinguishable predictive performance compared to the best model as\n\\[ \\left\\{ M_l: 0 \\in \\left[\\Delta \\widehat{\\textrm{elpd}}^l \\pm 2 \\widehat{\\text{se}}\\left(\\Delta \\widehat{\\textrm{elpd}}^l\\right)\\right] \\right\\}_{l=1, \\cdots, L \\leq K}.\\] To assess the reliability of the estimates for elpd, we count the number of Pareto-\\(\\hat{k}\\) diagnostics \\(&gt; 0.7\\) for each of the models.\n\n# add sum of Pareto k's &gt; 0.7 for all models with default LOO ####\ncomparisons_df &lt;- merge(\n  comparisons_df, \n  purrr::map_dbl(purrr::map(loos_default, ~.x$diagnostics$pareto_k), ~sum(.x&gt;0.7)),\n  by=\"row.names\") \n\n# set rownames to model names for merging\nrownames(comparisons_df) &lt;- comparisons_df$Row.names\n# select everything despite Row.names\ncomparisons_df &lt;- comparisons_df[2:length(comparisons_df)]\n# set descriptive name for new column \ncolnames(comparisons_df)[ncol(comparisons_df)] &lt;- \"n_high_pareto_ks\"\n\n# add loo comparison table with default LOO ####\nfull_df = merge(initial_multiverse, comparisons_df, by=0)\n# set row names to model names\nrownames(full_df) &lt;- full_df$Row.names\n# select everything despite Row.names\nfull_df = full_df[2:length(full_df)]\n\nWe visualise differences in estimated elpd and associated standard errors for all models and the remaining set of models indistinguishable by predictive performance. Models coloured in red are models with one or more Pareto-\\(\\hat{k}\\) greater than 0.7.\n\n\nCode\n# settings for all plots\ntheme_set(theme_classic() +\n            theme(legend.position = \"none\", \n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  strip.background = element_blank(),\n                  panel.background = element_blank(), \n                  text = element_text(size=8),\n                  plot.title = element_text(size=8),\n                  axis.title = element_text(size=8),\n                  axis.text = element_text(size=8)))\n\n# prepare data for plotting \ndf_plot &lt;- full_df |&gt;\n  mutate(high_pareto_ks = ifelse(n_high_pareto_ks &gt; 0, \"yes\", \"no\")) |&gt;\n  arrange(elpd_diff) |&gt;\n  mutate(model_id = forcats::fct_inorder(model_id)) |&gt;\n  select(modelnames, family, elpd_diff, se_diff, n_high_pareto_ks, model_id, high_pareto_ks)\n  \n# create plot for all models\nplot_elpddiffs &lt;- \n  ggplot(data = df_plot, aes(elpd_diff, model_id, col = high_pareto_ks, shape = family)) +\n  geom_pointrange(aes(xmin=elpd_diff-se_diff, xmax=elpd_diff+se_diff), fatten = .5, size = 3) + \n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") + \n  labs(subtitle = \"All models\") + \n  ylab(\"Models\") + \n  xlab(TeX(\"$\\\\Delta \\\\widehat{elpd}$\")) +\n  scale_color_manual(values=c(\"yes\" = \"red\", \"no\" = \"black\")) + \n  scale_shape_manual(values=c(\"poisson\" = 1, \"negbinomial\" = 6))\n\n# create plot for filtered set of models \ndf_plot_filtered &lt;- df_plot |&gt;\n  filter(elpd_diff + 2*se_diff &gt;= 0) \n\nplot_elpddiffs_filtered &lt;- \n  ggplot(data = df_plot_filtered, aes(elpd_diff, model_id, col = high_pareto_ks, shape = family)) +\n  geom_pointrange(aes(xmin=elpd_diff-se_diff, xmax=elpd_diff+se_diff), fatten = .5, size = 3) + \n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") + \n  labs(subtitle = \"Filtered set of models\") + \n  xlab(TeX(\"$\\\\Delta \\\\widehat{elpd}$\")) +\n  scale_color_manual(values=c(\"yes\" = \"red\", \"no\" = \"black\")) + \n  scale_shape_manual(values=c(\"poisson\" = 1, \"negbinomial\" = 6)) + \n  theme(axis.title.y = element_blank())\n\nplot &lt;- plot_elpddiffs | plot_elpddiffs_filtered\nplot"
  },
  {
    "objectID": "casestudies/workflows-multiverse/index.html#filtering-with-posterior-predictive-checks",
    "href": "casestudies/workflows-multiverse/index.html#filtering-with-posterior-predictive-checks",
    "title": "Iterative filtering for multiverse analyses of treatment effects",
    "section": "Filtering with posterior predictive checks",
    "text": "Filtering with posterior predictive checks\nIn the left subplot, elpd results where Pareto-\\(\\hat k\\) diagnostic indicated unreliable computation for PSIS-LOO-CV are highlighted with red colour. Instead of using computationally more intensive CV approaches, we can use posterior predictive checking to rule out these models. For the given multiverse, all models with high Pareto-\\(\\hat k\\) assume a Poisson distribution as the distributional family for the observations.\n\n\nCode\n# helper function\nget_one_ecdf_overlay &lt;- function(df, y, model_char = \"\", fontsize=8){\n  # set ggplot theme\n  theme_set(theme_classic() +\n              theme(panel.grid.major = element_blank(),\n                    panel.grid.minor = element_blank(),\n                    strip.background = element_blank(),\n                    panel.background = element_blank(),\n                    text = element_text(size=fontsize),\n                    plot.title = element_text(size=fontsize),\n                    axis.title = element_text(size=fontsize),\n                    axis.text = element_text(size=fontsize)))\n  \n  # bayesplot colour scheme\n  bayesplot::color_scheme_set(\"gray\")\n  \n  # get predictions for one model \n  yrep &lt;- df |&gt;\n    filter(model_id == model_char) |&gt;\n    pull(ypred)\n  \n  # get model family\n  modelfamily &lt;- df |&gt;\n    filter(model_id == model_char) |&gt;\n    mutate(family = recode(family, \"poisson\" = \"Poisson\", \"negbinomial\" = \"Negative Binomial\")) |&gt;\n    pull(family)\n  \n  # get model name \n  modelname_long &lt;- df |&gt;\n    filter(model_id == model_char) |&gt;\n    pull(modelnames)\n  \n  # remove info on prior for plotting \n  modelname &lt;- substr(modelname_long,1,regexpr(\",\",modelname_long)-1)\n  \n  # create plot\n  plot &lt;- ppc_ecdf_overlay(y = y, yrep = yrep[[1]][1:100,], discrete = TRUE) +\n    scale_x_continuous(trans=\"pseudo_log\", \n                       breaks=c(0, 5, 20, 50, 100), \n                       limits=c(0,110)) +\n    labs(title = paste0(modelfamily))\n  \n  return(plot)\n}\n\n# create two example plots \nplot_ppc_ecdf_model_22 &lt;- get_one_ecdf_overlay(full_df, brms::epilepsy$count, model_char = \"Model 22\") +\n  theme(legend.position=\"none\")\nplot_ppc_ecdf_model_21 &lt;- get_one_ecdf_overlay(full_df, brms::epilepsy$count, model_char = \"Model 21\") + \n  theme(axis.text.y = element_blank())\n\n# arrange \nplot_ppc_ecdf_model_22_21 &lt;- plot_ppc_ecdf_model_22 | plot_ppc_ecdf_model_21\nplot_ppc_ecdf_model_22_21\n\n\n\n\n\n\n\n\n\nThe above plot shows posterior predictive checking results for the best performing model among models assuming a Poisson distribution (Model 21) and its counterpart that differs only with respect to the chosen distributional family for the observations (Model 22). The results suggest that the Poisson model is not an appropriate choice for this data."
  },
  {
    "objectID": "casestudies/workflows-multiverse/index.html#what-comes-next",
    "href": "casestudies/workflows-multiverse/index.html#what-comes-next",
    "title": "Iterative filtering for multiverse analyses of treatment effects",
    "section": "What comes next?",
    "text": "What comes next?\nIn part II of this case study, we will extend the filtered set of models by including more complex models and filter again. We will also show how we can use integrated PSIS-LOO-CV to obtain reliable estimates for elpd."
  },
  {
    "objectID": "casestudies/workflows-multiverse/index.html#appendix",
    "href": "casestudies/workflows-multiverse/index.html#appendix",
    "title": "Iterative filtering for multiverse analyses of treatment effects",
    "section": "Appendix",
    "text": "Appendix\n\nExisting work\n\nposterior calibration checks with Säilynoja, Bürkner, and Vehtari (2022)\nmodel comparison with \\(\\texttt{R}\\)-package loo (Vehtari et al. 2020)\nmultiverse analysis (Steegen et al. 2016) and multiverse \\(\\texttt{R}\\)-package (Sarma et al. 2021)\nexplorable multiverse analyses (Dragicevic et al. 2019)\ncreating multiverse analysis scripts, exploring results with Boba (Liu et al. 2021)\nsurvey of visualisation of multiverse analyses (Hall et al. 2022)\nmodular STAN (Bernstein, Vákár, and Wing 2020)\nmodelling multiverse analysis for machine learning (Bell et al. 2022)\n\nThe following figures show two variants of flowcharts for Bayesian workflows with different levels of detail1. The most apparent similarities are (1) the possibility to iterate when needed, and (2) connecting the tasks of modelling and checking and tending to computational issues.\n1 Another flowchart for Bayesian workflow can be found in Michael Betancourt’s blogpost “Principled Bayesian Workflow”.\n\n\n\n\n\n\n\n\n\n\n(a) Bayesian Workflow in (Gelman et al. 2020)\n\n\n\n\n\n\n\n\n\n\n\n(b) Bayesian Workflow in (Martin, Kumar, and Lao 2021).\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nChallenges in Bayesian workflows\n\nmulti-attribute multi-objective scenarios\nnavigating necessary vs. “nice-to-have” steps\nstopping criteria and sufficient exploration\niterative model building, while transparent and robust\ncommunicating results of multiple models\n\n\n\nTransparent Exploration with multiverse analysis\n\n\n\n\n\n\n\n\n\n\n\n(a) Multiverse analysis compared to other approaches, from (Dragicevic et al. 2019).\n\n\n\n\n\n\n\n\n\n\n\n(b) Multiverse analysis in Bayesian workflow in (Gelman et al. 2020).\n\n\n\n\n\n\n\nFigure 2\n\n\n\nMultiverse analysis provides a way to transparently define and fit several models at once (Steegen et al. (2016)). In a workflow that requires iterations, this could allow parallel exploration, thereby, increasing efficiency. On the other hand, this exploration of sets of models necessarily depends on researcher/data analyst/user choices, and is subject to computational and cognitive constraints.\nReasoning on a set of models can be challenging, and dependence structures and different weights of modelling choices are not immediately clear when confronted with a large collection of possible models (see e.g., Hall et al. (2022)).\n\n\nDifferences and structure in a set of models\nGiven a set of \\(m\\) models \\(\\mathcal{M} = \\{M_1, M_2, ..., M_m\\}\\), let \\(C_1, \\cdots, C_k\\) denote \\(k\\) different modelling choices. If, for example, \\(C_1 = \\{\\text{\"poisson\"}, \\text{\"negbinomial\"}\\}\\), \\(C_2 = \\{\\text{\"Trt\"}\\}\\) and \\(C_3 = \\{ \\text{\"no zAge\"}, \\text{\"zAge\"} \\}\\), one could draw networks of the resulting four models solely based on how much they differ in each of the conditions.\nBelow, the left-hand side shows one step differences, while the right-hand side includes two step differences for models created using the above modelling choices \\(C_1, C_2\\) and \\(C_3\\).\n\n\n\n\n\n\n\n\n\n\n\n\nD\n\n\n\nA\n\nPoisson(Trt)\n\n\n\nB\n\nPoisson(Trt+zAge)\n\n\n\nA--B\n\n\n\n\nC\n\nNegbinom(Trt)\n\n\n\nA--C\n\n\n\n\nD\n\nNegbinom(Trt+zAge)\n\n\n\nB--D\n\n\n\n\nC--D\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nD\n\n\n\nA\n\nPoisson(Trt)\n\n\n\nB\n\nPoisson(Trt+zAge)\n\n\n\nA--B\n\n\n\n\nC\n\nNegbinom(Trt)\n\n\n\nA--C\n\n\n\n\nD\n\nNegbinom(Trt+zAge)\n\n\n\nB--D\n\n\n\n\nC--B\n\n\n\n\n\n\nC--D\n\n\n\n\nD--A"
  },
  {
    "objectID": "casestudies/index.html",
    "href": "casestudies/index.html",
    "title": "Case studies",
    "section": "",
    "text": "Approximating leave-one-group-out cross-validation\n\n\n\npredictive model evaluation\n\n\nLOGO-CV\n\n\nbridge sampling\n\n\nPSIS\n\n\n\n\n\n\n\nAnna Elisabeth Riha\n\n\nJun 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIterative filtering for multiverse analyses of treatment effects\n\n\n\nmultiverse analysis\n\n\nBayesian workflows\n\n\nbrms\n\n\n\n\n\n\n\nAnna Elisabeth Riha\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogposts/index.html",
    "href": "blogposts/index.html",
    "title": "Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "casestudies/approx-logo-cv/index.html",
    "href": "casestudies/approx-logo-cv/index.html",
    "title": "Approximating leave-one-group-out cross-validation",
    "section": "",
    "text": "How can we approximate LOGO-CV reliably beyond 2D varying coefficients and for different model types?\nCode\n# load packages\nlibrary(here)\nlibrary(dplyr) \nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(parallel)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(loo) \nlibrary(bridgestan)\nlibrary(aghq)\n# set seed \nset.seed(424242)\n# cores \nnumberOfCores &lt;- detectCores() - 1 \n# plot theme \ntheme_set(theme_bw())"
  },
  {
    "objectID": "casestudies/approx-logo-cv/index.html#intro",
    "href": "casestudies/approx-logo-cv/index.html#intro",
    "title": "Approximating leave-one-group-out cross-validation",
    "section": "Intro",
    "text": "Intro\nLeave-one-group-out cross-validation is useful for estimating the expected predictive performance for new exchangeable groups. Commonly used cross-validation approximations via importance sampling are likely to fail if there are group-specific coefficients, as the posterior can change too much when all the group-specific observations are removed, leading to unreliable estimates. Pareto-smoothed importance sampling (PSIS) (Vehtari et al. 2024) alleviates some of these issues, but it has previously been observed that PSIS-LOO-CV can provide unreliable results when evaluating hierarchical models with a varying effect on the level of each observation. Integration over the target density has been investigated in the context of importance sampling and Bayesian LOO-CV as a means to stabilise approximations.\nFor instance, Vehtari et al. (2016), and Merkle, Furr, and Rabe-Hesketh (2019) present different approaches for integrating out latent model parameters to achieve better approximations for evaluating predictive performance. In case of a single coefficient per group, numerical 1D quadrature has been used successfully to improve the performance of the importance sampling cross-validation (see examples, e.g., in Aki Vehtari’s cross-validation demo using the roaches dataset or Riha et al. (2024)). We discuss alternative approaches to integrate out the group-specific coefficients, which scale well beyond 1D and work well with non-normal latent distributions. Our goal is to make the approach as automated as possible in brms (Bürkner 2017), a widely used package for Bayesian data analysis using the probabilistic programming language (Stan Development Team 2025)."
  },
  {
    "objectID": "casestudies/approx-logo-cv/index.html#dataset-verbagg",
    "href": "casestudies/approx-logo-cv/index.html#dataset-verbagg",
    "title": "Approximating leave-one-group-out cross-validation",
    "section": "Dataset verbAgg",
    "text": "Dataset verbAgg\nThe VerbAgg dataset available in the lme4 package (Bates et al. 2015) contains item responses to a questionnaire on verbal aggression with a subject identifier VerbAgg$id for each of the 316 participants with 24 observations each. The outcome of interest, VerbAgg$resp, is the subject’s response to the item as an ordered factor with levels no &lt; perhaps &lt; yes. To speed up the computation, we select only the first 20 participants for now.\n\ndata(\"VerbAgg\", package = \"lme4\")\n\n# make sure that no response levels can be dropped later on\nVerbAgg$r3 &lt;- as.numeric(VerbAgg$resp)\n\n# filter for 20 participants for illustration purposes \nVerbAgg_reduced &lt;- VerbAgg |&gt;\n  filter(id %in% 1:20) |&gt;\n  mutate(id = factor(id))"
  },
  {
    "objectID": "casestudies/approx-logo-cv/index.html#illustration-why-is-importance-sampling-hard-for-logo-cv",
    "href": "casestudies/approx-logo-cv/index.html#illustration-why-is-importance-sampling-hard-for-logo-cv",
    "title": "Approximating leave-one-group-out cross-validation",
    "section": "Illustration: Why is importance sampling hard for LOGO-CV?",
    "text": "Illustration: Why is importance sampling hard for LOGO-CV?\nTo illustrate that importance sampling can fail when target and proposal differ a lot, we use a simple example with one group-level parameter but more than one observation in each group by looking at a simpler version of the model that we fitted above. We compare the results for the full-data posterior, the leave-one-out posterior and the leave-one-group-out posterior.\nWe build an ordinal cumulative model using family = cumulative() in brms::brm() and, for simplicity, only include a varying intercept on the level of each group of observations for each individual.\n\n# full-data posterior \nfit_cumulative_simple &lt;- brm(\n  r3 ~ Gender + btype + mode + situ + (1 | id), \n  data = VerbAgg_reduced, \n  family = cumulative(),\n  chains = 4, cores = 4, warmup = 1000, iter = 2000,\n  file = here::here(\"data\", \"approx-logo-cv\", \"fit_verbagg_cumulative_simple\"),\n  init = 0 # all parameters are initialised to zero on the unconstrained space\n)\nposterior_draws &lt;- posterior::as_draws_df(fit_cumulative_simple)\n\nNow, we remove one observation to obtain the LOO posterior.\n\n# LOO posterior \nrow_id &lt;- 5\nVerbAgg_reduced_loo &lt;- VerbAgg_reduced[-row_id, ]\n  \nfit_cumulative_simple_loo &lt;- brm(\n  r3 ~ Gender + btype + mode + situ + (1 | id), \n  data = VerbAgg_reduced_loo, \n  family = cumulative(),\n  chains = 4, cores = 4, warmup = 1000, iter = 2000,\n  file = here::here(\"data\", \"approx-logo-cv\", \"fit_verbagg_cumulative_simple_loo\"),\n  init = 0 # all parameters are initialised to zero on the unconstrained space\n)\n\nposterior_draws_loo &lt;- posterior::as_draws_df(fit_cumulative_simple_loo)\n\nTo obtain the LOGO posterior, we fit a model without the data for grouping level 5 and use the posterior draws for the standard deviation of the varying intercept to obtain LOGO posterior samples.\n\n# LOGO posterior \nleave_out_id &lt;- 5 \nVerbAgg_reduced_logo &lt;- filter(VerbAgg_reduced, id != leave_out_id) \n\n# fit model without one of the groups \nfit_cumulative_simple_logo &lt;- brm(\n  r3 ~ Gender + btype + mode + situ + (1 | id), \n  data = VerbAgg_reduced_logo, \n  family = cumulative(),\n  chains = 4, cores = 4, warmup = 1000, iter = 2000,\n  file = here::here(\"data\", \"approx-logo-cv\", \"fit_verbagg_cumulative_simple_logo\"),\n  init = 0 # all parameters are initialised to zero on the unconstrained space\n)\n\n# extract posterior samples for varying intercept sd \nposterior_draws_logo &lt;- posterior::as_draws_df(fit_cumulative_simple_logo)\nsd_draws &lt;- posterior_draws_logo[, \"sd_id__Intercept\"]$sd_id__Intercept\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nSince we assume Normal priors, we can obtain the LOGO posterior results via rnorm(brms::ndraws(fit_cumulative_simple_logo), 0, sd = sd_draws). We store all the information in one data frame for easier plotting later on.\n\n# Create df for plotting \nname_coeff &lt;- paste0(\"r_id[\", leave_out_id, \",Intercept]\")\n\ndf_plot_posteriors_rs &lt;- as.data.frame(rbind(\n  tibble(r_values = posterior_draws[, \"r_id[5,Intercept]\"]$`r_id[5,Intercept]`, \n         posterior_type = rep(\"full-data posterior\", length = NROW(posterior_draws))), \n  tibble(r_values = posterior_draws_loo[, \"r_id[5,Intercept]\"]$`r_id[5,Intercept]`, \n         posterior_type = rep(\"LOO posterior\", length = NROW(posterior_draws_loo))), \n  tibble(r_values = rnorm(brms::ndraws(fit_cumulative_simple_logo), 0, sd = sd_draws), \n         posterior_type = rep(\"LOGO posterior\", length = brms::ndraws(fit_cumulative_simple_logo)))\n)) |&gt;\n  mutate(posterior_type = factor(posterior_type, levels = c(\"full-data posterior\", \"LOO posterior\", \"LOGO posterior\")))\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nWe visualise the three different posterior results, and observe that while the full-data posterior and the LOO posterior are rather close to one another, the LOGO posterior differs considerable from the two.\n\n# Visualise \ncustom_colors &lt;- c(\"#8B8D7A\",\"#D55E00\",\"#0072B2\")\n\nplot_posterior_rs_comparison &lt;- ggplot(data = df_plot_posteriors_rs, aes(x = r_values, color = posterior_type, fill = NULL)) +\n  geom_density(linewidth = 2) + \n  geom_vline(aes(xintercept = 0), size = 0.7, linetype = \"dashed\") + \n  scale_color_manual(values = custom_colors) +\n  xlab(\"Posterior results for rid[5,Intercept]\") +\n  theme_classic() +\n  theme(\n    legend.position = \"inside\",\n    legend.position.inside = c(0.8,0.7),\n    legend.title = element_blank(), \n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.line.y = element_blank()\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplot_posterior_rs_comparison"
  },
  {
    "objectID": "casestudies/approx-logo-cv/index.html#example-analysing-repsonses-to-a-verbal-aggression-questionnaire",
    "href": "casestudies/approx-logo-cv/index.html#example-analysing-repsonses-to-a-verbal-aggression-questionnaire",
    "title": "Approximating leave-one-group-out cross-validation",
    "section": "Example: Analysing repsonses to a verbal aggression questionnaire",
    "text": "Example: Analysing repsonses to a verbal aggression questionnaire\nWe now assume a more complex model with a varying effect based on the subject identifier id as well as a varying slope for the behaviour type btype (a factor with three levels curse, scold, shout).\n\nfit_cumulative &lt;- brm(\n  r3 ~ Gender + btype + mode + situ + (btype || id), \n  data = VerbAgg_reduced, \n  family = cumulative(),\n  chains = 4, \n  cores = 4, \n  iter = 2000,\n  file = here::here(\"data\", \"approx-logo-cv\", \"fit_verbagg_cumulative\"),\n  init = 0 # all parameters initialised to zero on unconstrained space\n)\n\n\nEvaluating predictions for a new observation with PSIS-LOO-CV\nWe can use PSIS-LOO-CV to evaluate the predictive performance of our model, but this only allows us to evaluate the predictive abilities for individual observations.\n\nloo_psis &lt;- loo(fit_cumulative)\nloo_psis\n\n\nComputed from 4000 by 480 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -376.0 15.5\np_loo        44.7  2.7\nlooic       752.0 31.0\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.6]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\n\n\nEvaluating predictions for a new individual\nWe can visualise the responses for all 20 participants and highlight that, in our modelling scenario, leaving one individual out means leaving one group of observations out.\n\n\nCode\nspecified_id &lt;- 5\nmax_resp &lt;- 3\ny_lower_bound &lt;- 0.5 \ny_upper_bound &lt;- 3.35 \n\ncustom_colors &lt;- c(\n  \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\",\n  \"#D55E00\", \"#CC79A7\", \"#999999\", \"#DDCC77\", \"#B0E0E6\",\n  \"#FFA07A\", \"#6495ED\", \"#E6A5CC\", \"#F4C842\", \"#AAF0D1\",\n  \"#61D0D1\", \"#D4A5E2\", \"#8B8D7A\", \"#FC84A9\", \"#A3A48F\"\n)\n\nplot_data_with_group &lt;- VerbAgg_reduced |&gt;\n  ggplot(aes(x = id, y = factor(r3), color = id)) +\n  geom_point(alpha = 0.2) + \n  geom_jitter(width = 0.2, height = 0.3) + \n  geom_rect(aes(xmin = specified_id - 0.5, \n                xmax = specified_id + 0.5, \n                ymin = y_lower_bound, \n                ymax = y_upper_bound), \n            fill = NA, color = \"black\", linetype = \"dashed\") +\n  annotate(\"text\", x = specified_id + 0.5, y = y_upper_bound + 0.12,\n           label = \"Leave-one-group-out\", size = 3, hjust = 0.5) +\n  scale_color_manual(values = custom_colors) +\n  theme(legend.position = \"none\") + \n  xlab(\"Group IDs\") + \n  ylab(\"Response\")\n\nplot_data_with_group\n\n\n\n\n\n\n\n\n\n\nBrute-force LOGO-CV\nFirst, we use k-fold CV with k equal to the number of groups in our data to compute the leave-one-group-out predictive distributions. We can use brms::kfold() to perform brute-force LOGO-CV. In particular, if the argument folds is NULL, but we specify group = \"id\", the data is split up into subsets, each time omitting all observations of one of the levels of VerbAgg::id, while ignoring argument K. To perform brute-force LOGO-CV, we need to evaluate the likelihood for each group. In our case, this means that we need to refit the model 316 times.\n\npath_logo_brute &lt;- here::here(\"data\", \"approx-logo-cv\", \"logo_brute_verbagg.rds\")\n\nif (file.exists(path_logo_brute)) {\n  \n  logo_brute &lt;- readRDS(path_logo_brute)\n\n} else {\n  \n  logo_brute &lt;- kfold(\n    fit_cumulative, \n    group = \"id\", \n    joint = TRUE, \n    chains = 1, \n    init = 0)\n}\n\n\nlogo_brute\n\n\nBased on 20-fold cross-validation.\n\n           Estimate   SE\nelpd_kfold   -401.4 27.1\np_kfold        64.3  4.8\nkfoldic       802.9 54.2\n\n\n\n\nPSIS-LOGO-CV\nNow, we compute the leave-one-group-out predictive distributions using Pareto-smoothed importance sampling (PSIS).\n\n# matrix of pointwise log likelihood values\nlog_lik_pointwise &lt;- log_lik(fit_cumulative)\n\n# 20 individuals answered 24 question items each \ngids &lt;- rep(1:20, times = 24) \n  \n# grouped log likelihood values\nlog_lik_g &lt;- t(apply(log_lik_pointwise, 1, function(row) {\n  tapply(row, INDEX = gids, FUN = sum)\n}))\n\nlogo_psis &lt;- loo(log_lik_g)\n\nWarning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.\n\n# compute log ratios from grouped log likelihood values \nlog_ratios &lt;- -1 * log_lik_g\nr_eff &lt;- relative_eff(\n  exp(-log_ratios), # exp(-log_ratios) is the reciprocal of the importance ratios\n  chain_id = rep(1:nchains(fit_cumulative), each = ndraws(fit_cumulative) / nchains(fit_cumulative))) \n\npsis_logo &lt;- psis(log_ratios, r_eff = r_eff)\n\nWarning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.\n\npsis_logo\n\nComputed from 4000 by 20 log-weights matrix.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.1]).\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)      0     0.0%   &lt;NA&gt;    \n   (0.7, 1]   (bad)      10    50.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad) 10    50.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nWe can visualise the Pareto \\(\\hat{k}\\) values which are the estimated shape parameters of the generalised Pareto distribution that is fitted to the tails of the distribution of importance weights. Whenever we observe values above the threshold \\(0.7\\), this informs us that the estimates obtained with Pareto-smoothed importance sampling are not reliable (see, e.g., Vehtari et al. (2024)).\n\n\nCode\nplot_data &lt;- data.frame(\n  pareto_k_hats = psis_logo$diagnostics$pareto_k, \n  group_ids = seq_along(psis_logo$diagnostics$pareto_k)\n)\n\nplot_pareto_k_psis_logo &lt;- \n  ggplot(data = plot_data, aes(x = group_ids, y = pareto_k_hats)) +\n  geom_point(shape = 3, color = \"darkblue\") +\n  geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"darkred\") +\n  scale_y_continuous(breaks = seq(0, max(plot_data$pareto_k_hats)+0.1, by = 0.1), limits = c(0, 1.4)) + \n  ylab(\"\") +\n  xlab(\"Group IDs\") +\n  ggtitle(\"Unreliable results with PSIS-LOGO-CV\") \n\nplot_pareto_k_psis_logo\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_data_2 &lt;- data.frame(\n  pareto_k_hats = logo_psis$diagnostics$pareto_k, \n  group_ids = seq_along(logo_psis$diagnostics$pareto_k)\n)\n\nplot_pareto_k_logo_psis &lt;- \n  ggplot(data = plot_data_2, aes(x = group_ids, y = pareto_k_hats)) +\n  geom_point(shape = 3, color = \"darkblue\") +\n  geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"darkred\") +\n  scale_y_continuous(breaks = seq(0, max(plot_data$pareto_k_hats)+0.1, by = 0.1), limits = c(0, 1.4)) + \n  ylab(\"\") +\n  xlab(\"Group IDs\") +\n  ggtitle(\"Unreliable results with PSIS-LOGO-CV\") \n\nplot_pareto_k_logo_psis\n\n\n\n\n\n\n\n\n\nPlain PSIS-LOGO-CV has issues. We need to stabilise the importance weights by modifying the log-likelihood computation using, for example, integration.\n\n\n\nApproximate LOGO-CV\nFirst, we need to extract information from the brmsfit object and set up an empty fit object that will be updated later on.\nThe below custom function setup_fun() stores the linear predictor excluding the varying effects, extracts the relevant posterior draws for the varying effects, and creates an empty brmsfit object that will be updated when performing approximate LOGO-CV. Unfold the below code chunk to see the details.\n\n\nCode\nsetup_fun &lt;- function(fit_object, group_name, thin_draws_by = 1){\n  \n  stopifnot(is.brmsfit(fit_object))\n  # extract posterior draws without group-level effects\n  draws_linpred &lt;- brms::posterior_linpred(fit_object, re_formula = NA)\n  # extract sd terms\n  draws_sd &lt;- as.matrix(fit_object, variable = paste0(\"^sd_\", group_name, \"__\"), regex = TRUE)\n  # extract intercept terms \n  draws_int &lt;- as.matrix(fit_object, variable = \"^b_Intercept\", regex = TRUE)\n  \n  ids_draws &lt;- seq(1, posterior::ndraws(fit_object), by = thin_draws_by)\n  \n  # extract all group ids\n  ids_groups &lt;- unique(fit_object$data[group_name])[,1]  \n  # create list of all obs per group \n  ids_per_group &lt;- lapply(ids_groups, \\(i){\n    which(fit_object$data[group_name][,1] == i)\n  })\n  \n  # create initial placeholder for group-specific marginal df\n  outcome &lt;- all.vars(fit_object$formula$formula)[1]\n  data_marg_init &lt;- fit_object$data |&gt;\n    # select only one subject/group for marginal df \n    filter(get({{group_name}}) == unique(get({{group_name}}))[1]) |&gt; \n    mutate(plin = 0)\n  \n  # get model formula for empty fit \n  varying_part &lt;- regmatches(fit_object$formula$formula, regexpr(\"+ \\\\((.*) (.*)\\\\)\", fit_object$formula$formula))\n  if (fit_object$family$family == \"cumulative\"){\n    outcome_levels &lt;- as.numeric(length(unique(fit_object$data[,outcome])))\n    thresholds &lt;- outcome_levels - 1 \n    empty_model_formula &lt;- brms::brmsformula(formula = glue::glue(\"{outcome} | thres({thresholds}) ~ 1 + offset(plin) + {varying_part}\"))\n  } else {\n    empty_model_formula &lt;- brms::brmsformula(formula = glue::glue(\"{outcome} ~ 0 + offset(plin) + {varying_part}\"))\n  }\n  \n  # placeholder for prior for varying effects set to constant\n  constant_prior &lt;- \n    prior(constant(tau, broadcast = FALSE), class = \"sd\", group = \"id\") + \n    prior(constant(alpha, broadcast = FALSE), class = \"Intercept\") \n  stanvars &lt;- \n    stanvar(rep(1.0, ncol(draws_sd)), name = \"tau\") +\n    stanvar(rep(1.0, ncol(draws_int)), name = \"alpha\")\n\n  # create empty fit using constant prior and stanvars\n  fit_marg_empty &lt;- brms::brm(formula = empty_model_formula, \n                              data = data_marg_init, \n                              family = fit_object$family,\n                              prior = constant_prior,\n                              stanvars = stanvars, \n                              save_pars = save_pars(all = TRUE),\n                              seed = 424242,\n                              chains = 0)\n  \n  # return setup\n  list(data_marg_init = data_marg_init, \n       fit_object = fit_object, \n       empty_model_formula = empty_model_formula, \n       fit_marg_empty = fit_marg_empty, \n       draws_linpred = draws_linpred, \n       draws_sd = draws_sd,\n       draws_int = draws_int, \n       ids_groups = ids_groups, \n       ids_per_group = ids_per_group, \n       ids_draws = ids_draws)\n}\n\n\nNow, we use our custom function to set everything up for the next steps:\n\nsetup &lt;- setup_fun(fit_object = fit_cumulative, group_name = \"id\") \n\nCompiling Stan program...\n\n\nStart sampling\n\n\nthe number of chains is less than 1; sampling not done\n\n\nAdditionally, we will also separately store the group name, the group ids and the draw ids for the fit object:\n\ngroup_name &lt;- \"id\"\ngroup_ids &lt;- unique(fit_cumulative$data[group_name])[,1]  \ndraw_ids &lt;- seq(1, ndraws(fit_cumulative))\n\n\nBridgesampling + PSIS-LOGO-CV\nWe first set up a helper function that takes in our previously generated setup for our fit object fit_cumulative and returns one log likelihood value.\n\nbridgesampling_fun &lt;- function(setup, group_name, group_id, draw_id){\n  \n  # extract what is needed from setup \n  fit_object &lt;- setup$fit_object\n  fit_marg_empty &lt;- setup$fit_marg_empty\n  ids_one_group &lt;- setup$ids_per_group[[group_id]]\n  draw_sd &lt;- setup$draws_sd[draw_id,]\n  draw_int &lt;- setup$draws_int[draw_id,]\n  draw_linpred &lt;- setup$draws_linpred[draw_id, ids_one_group]\n  \n  # data input for updating the empty marginal fit object \n  data_marg &lt;- fit_object$data |&gt;\n    filter(id == group_id) |&gt;\n    mutate(plin = draw_linpred) \n  \n  # update empty marginal fit object \n  fit_marg &lt;- update(\n    fit_marg_empty, \n    newdata = data_marg, \n    stanvars = stanvar(draw_sd, name = \"tau\") + \n      stanvar(draw_int, name = \"alpha\"),\n    chains = 1, \n    warmup = 200, \n    iter = 1000, \n    silent = 2,\n    init = 0,\n    refresh = 0\n  )\n  \n  # set seed again for bridge sampling \n  set.seed(424242)\n  \n  log_lik_marg_one_group_one_draw &lt;- brms::bridge_sampler(fit_marg, silent = TRUE, use_neff = FALSE)$logml\n  return(log_lik_marg_one_group_one_draw)\n}\n\nNow, we apply our helper function for bridge sampling in a loop over group levels and draws to approximate the grouped log likelihood values using bridge sampling:\n\npath_logo_bridge_parallel &lt;- here::here(\"data\", \"approx-logo-cv\", \"logo_bridge_verbagg_parallel.rds\")\n\nif (file.exists(path_logo_bridge_parallel)) {\n  \n  logo_bridge_parallel_groups &lt;- readRDS(path_logo_bridge_parallel)\n\n} else {\n  \n  log_lik_grouped_bridge_parallel &lt;- \n    lapply(group_ids, \\(g_id){\n      mclapply(draw_ids, \\(d_id, mc.cores = numberOfCores){\n        bridgesampling_fun(setup, group_name, g_id, d_id)\n        })\n      })\n  \n  # format as array ####\n  number_of_chains &lt;- 1 \n  log_lik_array_bridge_parallel_groups &lt;- array(\n    unlist(log_lik_grouped_bridge_parallel), \n    dim = c(length(draw_ids), number_of_chains, length(group_ids)))\n  \n  # set dimnames of array ####\n  dimnames(log_lik_array_bridge_parallel_groups) &lt;- list(\n    iteration = seq(length(draw_ids)),\n    chain = seq(number_of_chains),\n    variable =  paste0(\"log_lik[\", group_ids, \"]\"))\n  \n  # convert into draws array ####\n  log_lik_array_bridge_parallel_groups &lt;-\n    posterior::as_draws(log_lik_array_bridge_parallel_groups)\n  \n  # LOO with log likelihood by groups ####\n  logo_bridge_parallel_groups &lt;- loo(log_lik_array_bridge_parallel_groups, r_eff = NA)\n  saveRDS(logo_bridge_parallel_groups, path_logo_bridge_parallel)\n}\n\n\nlogo_bridge_parallel_groups\n\n\nComputed from 4000 by 20 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -400.6 27.2\np_loo        18.0  2.2\nlooic       801.3 54.4\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nWe visualise the Pareto-\\(\\hat{k}\\) values, and observe that, in contrast to the results obtained with PSIS-LOGO-CV, they are all below the threshold of \\(0.7\\), that is, the estimates are all reliable.\n\n\nCode\nplot_data_pareto_k_bridge &lt;- data.frame(\n  pareto_k_hats = logo_bridge_parallel_groups$diagnostics$pareto_k, \n  group_ids = seq_along(logo_bridge_parallel_groups$diagnostics$pareto_k)\n)\n\nplot_poster_pareto_k_bridge &lt;- \n  ggplot(data = plot_data_pareto_k_bridge, aes(x = group_ids, y = pareto_k_hats)) +\n  geom_point(shape = 3, color = \"darkblue\") +\n  geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"darkred\") +\n  scale_y_continuous(breaks = seq(0, 1.4, by = 0.1), limits = c(0, 1.4)) + \n  ylab(\"\") +\n  xlab(\"Group IDs\") +\n  ggtitle(\"Bridge sampling + PSIS-LOGO-CV\") \n\nplot_poster_pareto_k_bridge\n\n\n\n\n\n\n\n\n\n\n\nLaplace approximation + PSIS-LOGO-CV\nWe first set up a helper function that takes in our previously generated setup for our fit object fit_cumulative and returns a single entry of the log likelihood marginal using Laplace approximation.\n\n## LOGO-CV with Laplace approximation \nlaplace_fun &lt;- function(setup, group_name, group_id, draw_id){\n  \n    # extract what is needed from setup \n    fit_object &lt;- setup$fit_object\n    empty_model_formula &lt;- setup$empty_model_formula\n    fit_marg_empty &lt;- setup$fit_marg_empty\n    ids_one_group &lt;- setup$ids_per_group[[group_id]]\n    draw_sd &lt;- setup$draws_sd[draw_id,]\n    draw_int &lt;- setup$draws_int[draw_id,]\n    draw_linpred &lt;- setup$draws_linpred[draw_id, ids_one_group]\n    \n    # compile empty model\n    bsso_marg &lt;- bridgestan::compile_model(cmdstanr::write_stan_file(stancode(fit_marg_empty)))\n    \n    # prepare data \n    data_one_group &lt;- fit_object$data |&gt; filter(get({{group_name}}) == group_id)\n    data_marg &lt;- data_one_group |&gt; mutate(plin = draw_linpred) \n    \n    data_for_stan &lt;- make_standata(\n      empty_model_formula, \n      data = data_marg, \n      family = fit_object$family,\n      prior = prior(constant(tau, broadcast = FALSE), \"sd\", group = \"id\") +\n        prior(constant(alpha, broadcast = FALSE), \"Intercept\"),\n      stanvars = stanvar(1.0, name = \"tau\") +\n        stanvar(1.0, name = \"alpha\"))\n    \n    # update draw specific part of the data list\n    data_for_stan$offsets &lt;- array(draw_linpred)\n    data_for_stan$tau &lt;- draw_sd\n    data_for_stan$alpha &lt;- draw_int\n    \n    # instantiate the model with updated data\n    suppressWarnings(bsm &lt;- StanModel$new(lib = bsso_marg,\n                                          data = to_stan_json(data_for_stan),\n                                          seed = draw_id))\n    \n    # log-density, gradient, and hessian functions\n    ffs &lt;- list(fn=bsm$log_density,\n                gr=\\(x) {bsm$log_density_gradient(x)$gradient},\n                he=\\(x) {bsm$log_density_hessian(x)$hessian})\n    \n    # initial unconstrained parameter values for optimization\n    th0 &lt;- rep(0,3)\n    \n    # Laplace approximation ####\n    aghq_laplace_marg &lt;- aghq::laplace_approximation(ffs, startingvalue = th0)\n    log_lik_marg_one_group_one_draw &lt;-  get_log_normconst(aghq_laplace_marg)\n    return(log_lik_marg_one_group_one_draw)\n}\n\nTo instantiate the model with updated data in our above helper function, we need an additional helper function that returns the JSON literal of our Stan model. We use a modified version of cmdstanr::write_stan_json from the cmdstanr package (Gabry et al. 2025).\n\n\nCode\n# helper function to return JSON literal modified from cmdstanr::write_stan_json\n\nto_stan_json &lt;- function(data, always_decimal = FALSE) {\n  if (!is.list(data)) {\n    stop(\"'data' must be a list.\", call. = FALSE)\n  }\n  \n  data_names &lt;- names(data)\n  if (length(data) &gt; 0 &&\n      (length(data_names) == 0 ||\n       length(data_names) != sum(nzchar(data_names)))) {\n    stop(\"All elements in 'data' list must have names.\", call. = FALSE)\n    \n  }\n  if (anyDuplicated(data_names) != 0) {\n    stop(\"Duplicate names not allowed in 'data'.\", call. = FALSE)\n  }\n  \n  for (var_name in data_names) {\n    var &lt;- data[[var_name]]\n    if (!(is.numeric(var) || is.factor(var) || is.logical(var) ||\n          is.data.frame(var) || is.list(var))) {\n      stop(\"Variable '\", var_name, \"' is of invalid type.\", call. = FALSE)\n    }\n    if (anyNA(var)) {\n      stop(\"Variable '\", var_name, \"' has NA values.\", call. = FALSE)\n    }\n    \n    if (is.table(var)) {\n      var &lt;- unclass(var)\n    } else if (is.logical(var)) {\n      mode(var) &lt;- \"integer\"\n    } else if (is.data.frame(var)) {\n      var &lt;- data.matrix(var)\n    } else if (is.list(var)) {\n      var &lt;- list_to_array(var, var_name)\n    }\n    data[[var_name]] &lt;- var\n  }\n  \n  # unboxing variables (N = 10 is stored as N : 10, not N: [10])\n  jsonlite::toJSON(\n    data,\n    auto_unbox = TRUE,\n    factor = \"integer\",\n    always_decimal = always_decimal,\n    digits = NA,\n    pretty = TRUE\n  )\n}\n\n\nNow, we loop over grouping levels and draws with our helper function laplace_fun() defined above to obtain estimates of our log likelihood values:\n\npath_logo_laplace_parallel &lt;- here::here(\"data\", \"approx-logo-cv\", \"logo_laplace_verbagg_parallel.rds\") \n\nif (file.exists(path_logo_laplace_parallel)) {\n  \n  logo_laplace_parallel_groups &lt;- readRDS(path_logo_laplace_parallel)\n\n} else {\n  \n  log_lik_grouped_laplace_parallel &lt;- \n    lapply(group_ids, \\(g_id){\n      mclapply(draw_ids, \\(d_id, mc.cores = numberOfCores){\n        laplace_fun(setup, group_name, g_id, d_id)\n        })\n      })\n  \n  # format as array ####\n  number_of_chains &lt;- 1\n  log_lik_array_laplace_parallel_groups &lt;- array(\n    unlist(log_lik_grouped_laplace_parallel), \n    dim = c(length(draw_ids), number_of_chains, length(group_ids)))\n\n  # set dimnames of array ####\n  dimnames(log_lik_array_laplace_parallel_groups) &lt;- list(\n    iteration = seq(length(draw_ids)),\n    chain = seq(number_of_chains),\n    variable =  paste0(\"log_lik[\", group_ids, \"]\"))\n\n  # convert into draws array ####\n  log_lik_array_laplace_parallel_groups &lt;-\n    posterior::as_draws(log_lik_array_laplace_parallel_groups)\n\n  # LOO with log likelihood by groups ####\n  logo_laplace_parallel_groups &lt;- loo(log_lik_array_laplace_parallel_groups, r_eff = NA)\n  saveRDS(logo_laplace_parallel_groups, path_logo_laplace_parallel)\n}\n\n\nlogo_laplace_parallel_groups\n\n\nComputed from 400 by 20 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -400.9 27.2\np_loo        17.4  2.1\nlooic       801.8 54.4\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nPareto k diagnostic values:\n                          Count Pct.    Min. ESS\n(-Inf, 0.62]   (good)     17    85.0%   76      \n   (0.62, 1]   (bad)       3    15.0%   &lt;NA&gt;    \n    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nWe visualise the Pareto-\\(\\hat{k}\\) values again, and observe that, in contrast to the results obtained with PSIS-LOGO-CV, they are largely below the threshold of \\(0.7\\), but, in contrast to bridge sampling, there are still few values above \\(0.7\\).\n\n\nCode\nplot_data_pareto_k_laplace &lt;- data.frame(\n  pareto_k_hats = logo_laplace_parallel_groups$diagnostics$pareto_k, \n  group_ids = seq_along(logo_laplace_parallel_groups$diagnostics$pareto_k)\n)\n\nplot_poster_pareto_k_laplace &lt;- \n  ggplot(data = plot_data_pareto_k_laplace, aes(x = group_ids, y = pareto_k_hats)) +\n  geom_point(shape = 3, color = \"darkblue\") +\n  geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"darkred\") +\n  scale_y_continuous(breaks = seq(0, 1.4, by = 0.1), limits = c(0, 1.4)) + \n  ylab(\"\") +\n  xlab(\"Group IDs\") +\n  ggtitle(\"Laplace + PSIS-LOGO-CV\") \n\nplot_poster_pareto_k_laplace\n\n\n\n\n\n\n\n\n\n\n\n\nComparing to brute-force LOGO-CV\nWe compare the results obtained with brute-force LOGO-CV to the results obtained with PSIS-LOGO-CV as well as our new approaches using bridge sampling and Laplace approximation in combination with PSIS-LOGO-CV.\n\nplot_df_logos &lt;-\n  data.frame(group_id = group_ids,\n             brute = logo_brute$pointwise[,\"elpd_kfold\"],\n             psis = logo_psis$pointwise[, \"elpd_loo\"],\n             bridgesampling = logo_bridge_parallel_groups$pointwise[,\"elpd_loo\"],\n             laplace = logo_laplace_parallel_groups$pointwise[,\"elpd_loo\"])\n\n\n\nCode\nplot_poster_psis_vs_brute &lt;-\n  ggplot(data = plot_df_logos, aes(x = psis, y = brute)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  labs(x = \"PSIS-LOGO-CV\",\n       y = \"brute-force LOGO-CV\")\n\nplot_poster_psis_vs_brute\n\n\n\n\n\n\n\n\n\nWe see that results using PSIS-LOGO-CV differ considerably from brute-force LOGO-CV results, indicating considerably lower accuracy in the log likelihood values obtained with default PSIS-LOGO-CV.\n\n\nCode\nplot_poster_bridge_vs_brute &lt;-\n  ggplot(data = plot_df_logos, aes(x = bridgesampling, y = brute)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  labs(x = \"Bridge sampling + PSIS-LOGO-CV\",\n       y = \"brute-force LOGO-CV\")\n\nplot_poster_laplace_vs_brute &lt;-\n  ggplot(data = plot_df_logos, aes(x = laplace, y = brute)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  labs(x = \"Laplace + PSIS-LOGO-CV\",\n       y = \"brute-force LOGO-CV\")\n\nplot_poster_bridge_vs_brute | plot_poster_laplace_vs_brute\n\n\n\n\n\n\n\n\n\nThe approaches using bridge sampling and Laplace approximation lead to similarly high accuracy in comparison to the brute-force results, and are closer to the brute-force results compared to PSIS-LOGO-CV."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anna Elisabeth Riha",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n\n\n\nI am a doctoral researcher in the Probabilistic Machine Learning group at Aalto University supervised by Aki Vehtari and Antti Oulasvirta.\nMy research interests lie in Bayesian data analysis, modelling workflows, iteration in model building and metastatistics. I am interested in interdisciplinary perspectives on Bayesian workflows and building bridges between method development and applied research.\nOn this website, you will find information about my research projects as well as case studies and a blog with occasional random musings. Please feel free to contact me if you have any questions or would like to discuss potential projects.\nBefore my current position, I completed my MSc in Statistics at Humboldt University Berlin with a thesis project on hyperprior sensitivity of wrapped Gaussian processes including an application to wind data, supervised by Nadja Klein and Thomas Kneib. During my studies, I also worked as a research assistant in the group of Andreas Brandmaier in the project “Formal Methods in Lifespan Psychology” at Max Planck Institute for Human Development in Berlin. Before, I earned my BSc degree in Economics with a focus on Empirical Economics and a minor in Sociology from University Potsdam where I wrote my Bachelor thesis under the supervision of Marco Caliendo."
  }
]