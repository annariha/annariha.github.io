---
title: "Bayesian Workflow and Multiverse Analysis"
author: "Anna Elisabeth Riha"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: references.bib
format: 
  html:
    toc: true
    toc-depth: 3
categories: [ideas, case study]
---

<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

</style>

```{r}
#| message: false
#| warning: false
#| include: false
library(knitr)
library(kableExtra)
library(tidyverse)
library(reactable)
library(htmltools)
```

<p class="comment">
How can we combine **transparent creation of sets of models** (multiverse analysis) with **recipes for model building and evaluation** (Bayesian workflow) to support decision making in Bayesian model building scenarios?
<p>

## Existing work

-   Bayesian workflow as introduced in @gelmanetal2020, also discussed in e.g., @martinetal2021 and a blogpost by Michael Betancourt[^2]
-   Bayesian model evaluation, and comparison (e.g., @vehtariojanen2012, @piironenvehtari2017)
-   prior and likelihood sensitivity checks in $\texttt{R}$-package $\texttt{priorsense}$[^3] [@kallioinen2021]
-   (posterior) calibration checks with @saeilynoja2022
-   model comparison with $\texttt{R}$-package $\texttt{loo}$ [@loo-package2022]
-   multiverse analysis [@steegenetal2016] and $\texttt{multiverse}$ $\texttt{R}$-package [@sarmaetal2021]
-   explorable multiverse analyses [@dragicevic2019]
-   creating multiverse analysis scripts, exploring results with Boba [@liu2021]
-   survey of visualisation of multiverse analyses [@halletal2022]
-   modular STAN [@bernstein2020]
-   modelling multiverse analysis (for machine learning) [@belletal2022]

[^2]: <https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html#1_Questioning_Authority>

[^3]: <https://github.com/n-kall/priorsense>

The following figures show two variants of flowcharts for Bayesian workflows with different levels of detail as well as crucial and optional steps[^4]. Similarities are, for example, (1) iterating when needed, and (2) connecting model and computation.

[^4]: Another interesting variant of a flowchart for Bayesian workflow can be found in Michael Betancourt's [blogpost](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html#1_Questioning_Authority) "Principled Bayesian Workflow".

![Bayesian Workflow in [@gelmanetal2020]](figures/bayesworkflow-illu.png){#fig-bw-gelman height="5%"}

![Bayesian Workflow in [@martinetal2021].](figures/workflow-bayescompbook.png){#fig-bw-martin width="90%"}

## Challenges in Bayesian workflows

-   multi-attribute multi-objective scenarios
-   navigating necessary vs. "nice-to-have" steps
-   stopping criteria and sufficient exploration
-   iterative model building, while transparent and robust
-   communicating results of multiple models

::: {.column-margin}
**What is a "good enough" model?**[^5]

[^5]: This is connected to the concept of a reference model (e.g., @vehtariojanen2012 and for projection predictive inference [@projpred-package2022]) as well as ideas on Bayesian model taxonomy as outlined, for example, by @buerkneretal2022.

-   depends (to some extent) on objectives of the analysis
-   domain knowledge
- passing prior predictive checks
-   "good" priors
-   convergence of sampling algorithms
-   posterior calibration
- passing posterior predictive checks
-   predictive performance
-   explainability
- ...
:::

## Transparent Exploration 

![Multiverse analysis compared to other approaches, from [@dragicevic2019].](figures/multiverse-illu.png){#fig-mvd width="60%"}

::: {layout-ncol="2" layout-valign="top"}
![What is a multiverse analysis report? in [@halletal2022].](figures/multiverse-def.png){#fig-mvhall width="50%"}

![Multiverse analysis in Bayesian workflow in [@gelmanetal2020].](figures/multiverse-in-bw.png){#fig-mvbw width="30%"}
:::

Multiverse analysis provides a way to transparently define and fit several models at once (@steegenetal2016). This allows to define and investigate several models at once. 

In a workflow that requires iterations, this could allow parallel exploration, thereby, increasing efficiency. 
On the other hand, this exploration of sets of models necessarily depends on researcher/data analyst/user choices, and is subject to computational and cognitive constraints. 

Reasoning on a set of models can be challenging, and dependence structures and different weights of modelling choices are not immediately clear when confronted with a large collection of possible models (see e.g., @halletal2022). 

## Differences and structure in a set of models 

Given a set of $m$ models $\mathcal{M} = \{M_1, M_2, ..., M_m\}$, let $C_1, \cdots, C_k$ denote $k$ different modelling choices. If, for example, $C_1 = \{\text{"poisson"}, \text{"negbinomial"}\}$, $C_2 = \{\text{"Trt"}\}$ and $C_3 = \{ \text{"no zAge"}, \text{"zAge"} \}$, one could draw networks of the resulting four models solely based on how much they differ in each of the conditions. 

Below, the left-hand side shows one step differences, while the right-hand side includes two step differences for models created using the above modelling choices $C_1, C_2$ and $C_3$.

::: {layout-ncol="2"}
```{dot}
//| fig-width: 4
graph D {
  
  A [shape=box,label="Poisson(Trt)"]
  B [shape=box,label="Poisson(Trt+zAge)"]
  C [shape=box,label="Negbinom(Trt)"]
  D [shape=box,label="Negbinom(Trt+zAge)"]
  
  A -- B 
  A -- C 
  C -- D 
  B -- D 

}
```

```{dot}
//| fig-width: 3

graph D {
  
  A [shape=box, label="Poisson(Trt)"]
  B [shape=box, label="Poisson(Trt+zAge)"]
  C [shape=box, label="Negbinom(Trt)"]
  D [shape=box, label="Negbinom(Trt+zAge)"]
  
  A -- B -- D
  A -- C -- D
  C -- B [color="grey:invis:grey"]
  D -- A [color="grey:invis:grey"]

}
```
:::

## Filtering and Flagging

What models need further investigation (or can be safely excluded) due to, for example

-   prior-data conflict
-   convergence issues
-   calibration problems for posterior
-   problematic posterior predictive checks
-   low predictive performance

Filtering (or flagging) also serves as a tool to indicate that we are in fact at a certain decision point in the Bayesian workflow (e.g., if computation failed, go to "Addressing computational issues (5)", see @fig-bw-gelman).

## Case Study: `brms::epilepsy`

For this case study, we use the dataset `brms::epilepsy` with 236 observations containing information on the following 9 variables, originally from Thall and Vail (1990) and Breslow and Clayton (1993):[^17]

[^17]: More information, e.g. in [brms docs](https://paul-buerkner.github.io/brms/reference/epilepsy.html) and in a [vignette](https://cran.r-project.org/web/packages/bayesian/vignettes/GetStarted.html) introducing the $\texttt{R}$-package $\texttt{bayesian}$.

-   $\texttt{Age}$: age of patients in years
-   $\texttt{Base}$: seizure count at 8-weeks baseline
-   $\texttt{Trt}$: 0 or 1 indicating if patient received anti-convulsant therapy
-   $\texttt{patient}$: patient number
-   $\texttt{visit}$: session number from 1 (first visit) to 4 (last visit)
-   $\texttt{count}$: seizure count between two visits
-   $\texttt{obs}$: observation number (unique identifier for each observation)
-   $\texttt{zAge}$: Standardized Age
-   $\texttt{zBase}$: Standardized Base

First, we create a dataframe of combinations of modelling choices. Here, we consider two different observation families (`poisson` and `negbinomial`), two different prior settings (`brms` default setting and horseshoe prior with `df=3` for the population-level effects), different combinations of covariates, models with and without random effects on the level of each patient and/or visit, as well as models with and without interaction effect `zBase * Trt`. This leads to $96$ models.  

```{r}
#| eval: false

# load data ####
dat <- brms::epilepsy 
outcome_str <- "count"

# create dataframe of combinations of model components ####

# observation families
families <- list(poisson = poisson(), 
                 negbinomial = negbinomial())

# priors 
priors <- list(brms_default = NULL, 
               brms_horseshoe = set_prior("horseshoe(3)")
)

combinations_df <- expand.grid(
  family = names(families),
  prior = priors,
  # fixed effects 
  Trt = c("", "Trt"), 
  zBase = c("", "zBase"),
  zAge = c("", "zAge"),
  # random effects, no observation level r.e.
  patient = c("", "(1 | patient)"),
  visit = c("", "(1 | visit)")
)

combinations_df <- combinations_df |> 
  # add interaction effect in the rows where treatment was left out, (i.e., where Trt == "")
  mutate(zBaseTrt = factor(
    case_when(
      Trt == "Trt" ~ "",
      Trt == "" ~ "zBase * Trt"))) |> 
  # filter out rows with interaction and zBase
  filter(!(zBaseTrt == "zBase * Trt" & combinations_df$zBase == "zBase"))
```

Then, we can fit models in parallel and evaluate the list of models jointly. This allows us to obtain joint summaries of model evaluation metrics for all models. An example of such a summary table is given below with results for the six models with highest PBMA weights and no divergent transitions in the collection of all $96$ models.  

```{r}
#| message: false
#| warning: false
#| echo: false

kable_df_epi <- read_rds(here::here("case-studies", "epilepsy", "data", "prelim", "kable_df_epi.rds")) 

knitr::kable(kable_df_epi) %>%
  kableExtra::kable_styling(font_size = 11)
```
